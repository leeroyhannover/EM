{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### augmentation for the Unet structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15898260276\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15185342964033605916\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:8a:00.0, compute capability: 6.0\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the status of GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "\n",
    "[print(x) for x in local_device_protos if x.device_type == 'GPU']\n",
    "# gpu_device_name = tf.test.gpu_device_name()\n",
    "# print(gpu_device_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading \n",
    "\n",
    "import os\n",
    "import mrcfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm  # ! this might result into problem with 'object'\n",
    "import os\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/li52/.local/lib/python3.6/site-packages/mrcfile/mrcinterpreter.py:219: RuntimeWarning: Unrecognised machine stamp: 0x00 0x00 0x00 0x00\n",
      "  warnings.warn(str(err), RuntimeWarning)\n",
      " 67%|██████▋   | 2/3 [00:00<00:00,  9.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomo2_focalseries.mrc\n",
      "tomo3_focalseries.mrc\n",
      "tomo1_focalseries.mrc\n",
      "tomo2_groundtruth.mrc\n",
      "tomo3_groundtruth.mrc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tomo1_groundtruth.mrc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def readMRC(path):\n",
    "    with mrcfile.open(path, mode='r+', permissive=True) as mrc:\n",
    "        mrc.header.map = mrcfile.constants.MAP_ID # for synthetic data, need to generate ID\n",
    "        data = mrc.data\n",
    "    return data\n",
    "\n",
    "DATA_PATH = './synthetic/'  # in hemera, only use relative path\n",
    "data_ids = next(os.walk(DATA_PATH))[1]\n",
    "raw = []\n",
    "for n, id_ in tqdm(enumerate(data_ids), total=len(data_ids)):  \n",
    "    path = DATA_PATH + id_\n",
    "    datanames = os.listdir(path)\n",
    "    for dataname in datanames:\n",
    "        if os.path.splitext(dataname)[1] == '.mrc': # all .mrc under the path\n",
    "            temp = readMRC(path + \"/\" + dataname).astype(np.uint8)\n",
    "            raw.append(temp)\n",
    "            print(dataname)\n",
    "            \n",
    "focal = raw[0:3]\n",
    "GT = raw[3:len(raw)]\n",
    "del raw, temp, datanames, dataname, path, data_ids, n, id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 256, 256)\n",
      "(512, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# raw data sorting: padding, reshape\n",
    "\n",
    "import torchio as tio\n",
    "from patchify import patchify, unpatchify\n",
    "\n",
    "# training dataset raw\n",
    "train_raw = np.vstack(([focal[0], focal[1]]));train = train_raw[...,np.newaxis].transpose((3,1,2,0)); label = np.vstack(([GT[0], GT[1]]))#;label = train_label[...,np.newaxis]\n",
    "\n",
    "trainIO = tio.ScalarImage(tensor=train)\n",
    "target_shape = 256,256,512  # padding into the same size\n",
    "crop_pad = tio.CropOrPad(target_shape, padding_mode='mean'); resized = crop_pad(trainIO) # padding with mean\n",
    "train_padd = resized.numpy().transpose((3,1,2,0)); train_padd = train_padd[...,0]\n",
    "print(train_padd.shape);print(label.shape)\n",
    "\n",
    "# testing dataset raw\n",
    "X_test = focal[2]; X_test = X_test[...,np.newaxis].transpose((3,1,2,0))\n",
    "Y_test_label = GT[2]; Y_test_label = (Y_test_label > 0.5).astype(np.float) \n",
    "\n",
    "testIO = tio.ScalarImage(tensor=X_test)\n",
    "target_shape = 256,256,256\n",
    "crop_pad = tio.CropOrPad(target_shape, padding_mode='mean'); resized = crop_pad(testIO)\n",
    "test_padd = resized.numpy().transpose((3,1,2,0)); test_padd = test_padd[...,0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patchify the images\n",
    "\n",
    "def rawPatch(imageStack,patchPara):\n",
    "    all_img_patches = []\n",
    "\n",
    "    for img in range(imageStack.shape[0]):\n",
    "        large_image = imageStack[img]\n",
    "\n",
    "        patches_img = patchify(large_image, (patchPara['x'],patchPara['y']), step=patchPara['step'])  # no overlap\n",
    "\n",
    "        for i in range(patches_img.shape[0]):\n",
    "            for j in range(patches_img.shape[1]):\n",
    "\n",
    "                single_patch_img = patches_img[i,j,:,:]\n",
    "                single_patch_img = (single_patch_img.astype('float32')) / 255.  # remember to standarize into 0-1\n",
    "\n",
    "                all_img_patches.append(single_patch_img)\n",
    "    \n",
    "    return all_img_patches, patches_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 256, 3)\n",
      "(256, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    "patchPara = {'x': 256, 'y': 256, 'step':256}\n",
    "Xtrain_patches, _ = rawPatch(train_padd, patchPara);Xtrain_patches = np.stack((Xtrain_patches,)*3, axis=-1) # dock 3 times, the model expects 3 channel\n",
    "# print(Xtrain_patches.shape)\n",
    "\n",
    "Ytrain_patches, _ = rawPatch(label, patchPara);Ytrain_patches = np.expand_dims(Ytrain_patches, -1)\n",
    "Ytrain_patches = (Ytrain_patches > 0.5).astype(np.float) # binarize the data\n",
    "\n",
    "# test dataset\n",
    "Xtest_patches, patchSize = rawPatch(test_padd, patchPara);Xtest_patches = np.stack((Xtest_patches,)*3, axis=-1)\n",
    "Ytest_patches, _ = rawPatch(Y_test_label, patchPara);Ytest_patches = np.expand_dims(Ytest_patches, -1)\n",
    "Ytest_patches = (Ytest_patches > 0.5).astype(np.float)\n",
    "print(Xtest_patches.shape);print(Ytest_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet structure in blocks\n",
    "import tensorflow.keras as k\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from tensorflow.keras.optimizers import Adam  # ! notice here must write in tensorflow.keras\n",
    "from keras.layers import Activation, MaxPool2D, Concatenate\n",
    "\n",
    "\n",
    "def conv_block(input, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
    "    x = BatchNormalization()(x)   #Not in the original network. \n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)  #Not in the original network\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "#Encoder block: Conv block followed by maxpooling\n",
    "\n",
    "def encoder_block(input, num_filters):\n",
    "    x = conv_block(input, num_filters)\n",
    "    p = MaxPool2D((2, 2))(x)\n",
    "    return x, p   \n",
    "\n",
    "#Decoder block\n",
    "#skip features gets input from encoder for concatenation\n",
    "\n",
    "def decoder_block(input, skip_features, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
    "    x = Concatenate()([x, skip_features])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "#Build Unet using the blocks\n",
    "def build_unet(input_shape):\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024) #Bridge\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)  #Binary (can be multiclass)\n",
    "\n",
    "    model = Model(inputs, outputs, name=\"U-Net\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 256, 256, 64) 1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 256, 256, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 256, 256, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 256, 256, 64) 36928       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 256, 256, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 256, 256, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 64) 0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 128, 128, 128 73856       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128, 128, 128 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 128, 128, 128 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 128, 128, 128 147584      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 128, 128, 128 512         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 128, 128, 128 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 128)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 64, 64, 256)  295168      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 64, 64, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 64, 64, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 256)  590080      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 256)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 512)  1180160     max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 512)  2048        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 512)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 512)  2359808     activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 512)  2048        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 512)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 512)  0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 1024) 4719616     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 1024) 4096        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 1024) 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 1024) 9438208     activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 1024) 4096        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 1024) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 32, 32, 512)  2097664     activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 1024) 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 512)  2359808     activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 512)  2048        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 512)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 64, 64, 256)  524544      activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 64, 64, 512)  0           conv2d_transpose_2[0][0]         \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 256)  1024        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 64, 256)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 256)  590080      activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 256)  1024        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 256)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 128, 128, 128 131200      activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128, 256 0           conv2d_transpose_3[0][0]         \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 128, 128, 128 295040      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 128, 128, 128 512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 128, 128, 128 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 128, 128, 128 147584      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 128, 128, 128 512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 128, 128, 128 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 256, 256, 64) 32832       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 256, 256, 128 0           conv2d_transpose_4[0][0]         \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 256, 256, 64) 256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 256, 256, 64) 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 64) 36928       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 256, 256, 64) 256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 256, 256, 64) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 1)  65          activation_18[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 31,055,297\n",
      "Trainable params: 31,043,521\n",
      "Non-trainable params: 11,776\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "\n",
    "model = build_unet(input_shape)\n",
    "model.compile(optimizer=Adam(lr = 1e-3), loss='binary_crossentropy', metrics=['accuracy'])  # add Adam from Keras, not tensorflow.adam\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "\n",
    "seed=24  # gurantee the images and masks are the same with augmentaion\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# dictionary for augmentation\n",
    "img_data_gen_args = dict(rotation_range=90,\n",
    "                     width_shift_range=0.3,\n",
    "                     height_shift_range=0.3,\n",
    "#                      shear_range=0.5,\n",
    "#                      zoom_range=0.3,\n",
    "                     horizontal_flip=True,\n",
    "                     vertical_flip=True,\n",
    "                     fill_mode='reflect')\n",
    "\n",
    "mask_data_gen_args = dict(rotation_range=90,\n",
    "                     width_shift_range=0.3,\n",
    "                     height_shift_range=0.3,\n",
    "#                      shear_range=0.5,\n",
    "#                      zoom_range=0.3,\n",
    "                     horizontal_flip=True,\n",
    "                     vertical_flip=True,\n",
    "                     fill_mode='reflect',\n",
    "                     preprocessing_function = lambda x: np.where(x>0, 1, 0).astype(x.dtype)) \n",
    "#Binarize the output again. Because the new-generated images comes with interperated values\n",
    "\n",
    "image_data_generator = ImageDataGenerator(**img_data_gen_args)\n",
    "\n",
    "\n",
    "batch_size= 8\n",
    "\n",
    "X_train = Xtrain_patches; Y_train = Ytrain_patches;\n",
    "X_test = Xtest_patches; Y_test = Ytest_patches;\n",
    "image_generator = image_data_generator.flow(X_train, seed=seed, batch_size=batch_size)\n",
    "valid_img_generator = image_data_generator.flow(X_test, seed=seed, batch_size=batch_size) #Default batch size 32, if not specified here\n",
    "\n",
    "mask_data_generator = ImageDataGenerator(**mask_data_gen_args)\n",
    "mask_generator = mask_data_generator.flow(Y_train, seed=seed, batch_size=batch_size)  # the seed is same\n",
    "valid_mask_generator = mask_data_generator.flow(Y_test, seed=seed, batch_size=batch_size)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 另外的处理数据的方式\n",
    "# image_data_generator = ImageDataGenerator(**img_data_gen_args)\n",
    "# image_data_generator.fit(X_train, augment=True, seed=seed)\n",
    "\n",
    "# image_generator = image_data_generator.flow(X_train, seed=seed)\n",
    "# valid_img_generator = image_data_generator.flow(X_test, seed=seed)\n",
    "\n",
    "# mask_data_generator = ImageDataGenerator(**mask_data_gen_args)\n",
    "# mask_data_generator.fit(y_train, augment=True, seed=seed)\n",
    "# mask_generator = mask_data_generator.flow(y_train, seed=seed)\n",
    "# valid_mask_generator = mask_data_generator.flow(y_test, seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_image_mask_generator(image_generator, mask_generator):\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img, mask) in train_generator:\n",
    "        yield (img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator\n",
    "\n",
    "my_generator = my_image_mask_generator(image_generator, mask_generator)  # give out the image generator and mask at teh same time\n",
    "\n",
    "validation_datagen = my_image_mask_generator(valid_img_generator, valid_mask_generator)\n",
    "\n",
    "x = image_generator.next()\n",
    "y = mask_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "192/192 [==============================] - 70s 366ms/step - loss: 0.6836 - acc: 0.5753 - val_loss: 0.5913 - val_acc: 0.9235\n",
      "Epoch 2/200\n",
      "192/192 [==============================] - 62s 324ms/step - loss: 0.6807 - acc: 0.5784 - val_loss: 0.5623 - val_acc: 0.9911\n",
      "Epoch 3/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6804 - acc: 0.5790 - val_loss: 0.5457 - val_acc: 0.9880\n",
      "Epoch 4/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6803 - acc: 0.5791 - val_loss: 0.5346 - val_acc: 0.9965\n",
      "Epoch 5/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6803 - acc: 0.5790 - val_loss: 0.5481 - val_acc: 0.9930\n",
      "Epoch 6/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6803 - acc: 0.5791 - val_loss: 0.5409 - val_acc: 0.9946\n",
      "Epoch 7/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6803 - acc: 0.5789 - val_loss: 0.5577 - val_acc: 0.9716\n",
      "Epoch 8/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6802 - acc: 0.5793 - val_loss: 0.5418 - val_acc: 0.9900\n",
      "Epoch 9/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6802 - acc: 0.5792 - val_loss: 0.5382 - val_acc: 1.0000\n",
      "Epoch 10/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6802 - acc: 0.5794 - val_loss: 0.5743 - val_acc: 0.9897\n",
      "Epoch 11/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6802 - acc: 0.5792 - val_loss: 0.5512 - val_acc: 0.9780\n",
      "Epoch 12/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6802 - acc: 0.5793 - val_loss: 0.5730 - val_acc: 0.9900\n",
      "Epoch 13/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6800 - acc: 0.5797 - val_loss: 0.5419 - val_acc: 0.9946\n",
      "Epoch 14/200\n",
      "192/192 [==============================] - 64s 333ms/step - loss: 0.6800 - acc: 0.5797 - val_loss: 0.5357 - val_acc: 0.9963\n",
      "Epoch 15/200\n",
      "192/192 [==============================] - 64s 334ms/step - loss: 0.6799 - acc: 0.5800 - val_loss: 0.5693 - val_acc: 0.9955\n",
      "Epoch 16/200\n",
      "192/192 [==============================] - 64s 334ms/step - loss: 0.6800 - acc: 0.5797 - val_loss: 0.5511 - val_acc: 0.9977\n",
      "Epoch 17/200\n",
      "192/192 [==============================] - 64s 334ms/step - loss: 0.6799 - acc: 0.5800 - val_loss: 0.5520 - val_acc: 0.9997\n",
      "Epoch 18/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6799 - acc: 0.5799 - val_loss: 0.5290 - val_acc: 0.9854\n",
      "Epoch 19/200\n",
      "192/192 [==============================] - 63s 326ms/step - loss: 0.6799 - acc: 0.5798 - val_loss: 0.5678 - val_acc: 0.9936\n",
      "Epoch 20/200\n",
      "192/192 [==============================] - 63s 327ms/step - loss: 0.6798 - acc: 0.5799 - val_loss: 0.5261 - val_acc: 0.9991\n",
      "Epoch 21/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6796 - acc: 0.5804 - val_loss: 0.6036 - val_acc: 0.9702\n",
      "Epoch 22/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6798 - acc: 0.5800 - val_loss: 0.5575 - val_acc: 0.9975\n",
      "Epoch 23/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6796 - acc: 0.5804 - val_loss: 0.5447 - val_acc: 0.9875\n",
      "Epoch 24/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6796 - acc: 0.5803 - val_loss: 0.5409 - val_acc: 0.9978\n",
      "Epoch 25/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6795 - acc: 0.5806 - val_loss: 0.5570 - val_acc: 0.9943\n",
      "Epoch 26/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6795 - acc: 0.5805 - val_loss: 0.5521 - val_acc: 0.9937\n",
      "Epoch 27/200\n",
      "192/192 [==============================] - 63s 326ms/step - loss: 0.6794 - acc: 0.5805 - val_loss: 0.5676 - val_acc: 0.9942\n",
      "Epoch 28/200\n",
      "192/192 [==============================] - 63s 327ms/step - loss: 0.6793 - acc: 0.5806 - val_loss: 0.5636 - val_acc: 0.9880\n",
      "Epoch 29/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6793 - acc: 0.5808 - val_loss: 0.5591 - val_acc: 0.9883\n",
      "Epoch 30/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6792 - acc: 0.5808 - val_loss: 0.5594 - val_acc: 0.9933\n",
      "Epoch 31/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6791 - acc: 0.5810 - val_loss: 0.5327 - val_acc: 0.9971\n",
      "Epoch 32/200\n",
      "192/192 [==============================] - 63s 327ms/step - loss: 0.6793 - acc: 0.5806 - val_loss: 0.5988 - val_acc: 0.9806\n",
      "Epoch 33/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6793 - acc: 0.5805 - val_loss: 0.5666 - val_acc: 0.9916\n",
      "Epoch 34/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6792 - acc: 0.5807 - val_loss: 0.5445 - val_acc: 0.9987\n",
      "Epoch 35/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6790 - acc: 0.5811 - val_loss: 0.5300 - val_acc: 0.9914\n",
      "Epoch 36/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6790 - acc: 0.5807 - val_loss: 0.5867 - val_acc: 0.9935\n",
      "Epoch 37/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6790 - acc: 0.5809 - val_loss: 0.5568 - val_acc: 0.9963\n",
      "Epoch 38/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6790 - acc: 0.5808 - val_loss: 0.5324 - val_acc: 0.9990\n",
      "Epoch 39/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6791 - acc: 0.5807 - val_loss: 0.5726 - val_acc: 0.9921\n",
      "Epoch 40/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6789 - acc: 0.5810 - val_loss: 0.5437 - val_acc: 0.9979\n",
      "Epoch 41/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6787 - acc: 0.5813 - val_loss: 0.6395 - val_acc: 0.7046\n",
      "Epoch 42/200\n",
      "192/192 [==============================] - 64s 332ms/step - loss: 0.6786 - acc: 0.5815 - val_loss: 0.5561 - val_acc: 0.9808\n",
      "Epoch 43/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6787 - acc: 0.5811 - val_loss: 0.5622 - val_acc: 0.9920\n",
      "Epoch 44/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6786 - acc: 0.5813 - val_loss: 0.5516 - val_acc: 0.9528\n",
      "Epoch 45/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6787 - acc: 0.5811 - val_loss: 0.5627 - val_acc: 0.9900\n",
      "Epoch 46/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6788 - acc: 0.5810 - val_loss: 0.5391 - val_acc: 0.9962\n",
      "Epoch 47/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6785 - acc: 0.5813 - val_loss: 0.5985 - val_acc: 0.9660\n",
      "Epoch 48/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6788 - acc: 0.5811 - val_loss: 0.5357 - val_acc: 0.9982\n",
      "Epoch 49/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6785 - acc: 0.5812 - val_loss: 0.6064 - val_acc: 0.9678\n",
      "Epoch 50/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6783 - acc: 0.5817 - val_loss: 0.5433 - val_acc: 0.9903\n",
      "Epoch 51/200\n",
      "192/192 [==============================] - 63s 327ms/step - loss: 0.6785 - acc: 0.5814 - val_loss: 0.6197 - val_acc: 0.7985\n",
      "Epoch 52/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6784 - acc: 0.5815 - val_loss: 0.5458 - val_acc: 0.9950\n",
      "Epoch 53/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6783 - acc: 0.5816 - val_loss: 0.5628 - val_acc: 0.9928\n",
      "Epoch 54/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6783 - acc: 0.5815 - val_loss: 0.5380 - val_acc: 0.9933\n",
      "Epoch 55/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6782 - acc: 0.5815 - val_loss: 0.5516 - val_acc: 0.9870\n",
      "Epoch 56/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6783 - acc: 0.5815 - val_loss: 0.5652 - val_acc: 0.9963\n",
      "Epoch 57/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6783 - acc: 0.5816 - val_loss: 0.5347 - val_acc: 0.9985\n",
      "Epoch 58/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6781 - acc: 0.5816 - val_loss: 0.5564 - val_acc: 0.9963\n",
      "Epoch 59/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6779 - acc: 0.5818 - val_loss: 0.5459 - val_acc: 0.9898\n",
      "Epoch 60/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6779 - acc: 0.5819 - val_loss: 0.5622 - val_acc: 0.9938\n",
      "Epoch 61/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6779 - acc: 0.5820 - val_loss: 0.5380 - val_acc: 0.9979\n",
      "Epoch 62/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6778 - acc: 0.5820 - val_loss: 0.6020 - val_acc: 0.9321\n",
      "Epoch 63/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6782 - acc: 0.5814 - val_loss: 0.5689 - val_acc: 0.9888\n",
      "Epoch 64/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6780 - acc: 0.5818 - val_loss: 0.5481 - val_acc: 0.9915\n",
      "Epoch 65/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6776 - acc: 0.5822 - val_loss: 0.5292 - val_acc: 0.9914\n",
      "Epoch 66/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6779 - acc: 0.5817 - val_loss: 0.5413 - val_acc: 0.9965\n",
      "Epoch 67/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6776 - acc: 0.5821 - val_loss: 0.5441 - val_acc: 0.9884\n",
      "Epoch 68/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6777 - acc: 0.5819 - val_loss: 0.5606 - val_acc: 0.9847\n",
      "Epoch 69/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6779 - acc: 0.5819 - val_loss: 0.5582 - val_acc: 0.9909\n",
      "Epoch 70/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6781 - acc: 0.5816 - val_loss: 0.5543 - val_acc: 0.9968\n",
      "Epoch 71/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6776 - acc: 0.5820 - val_loss: 0.5559 - val_acc: 0.9894\n",
      "Epoch 72/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6778 - acc: 0.5819 - val_loss: 0.5404 - val_acc: 0.9962\n",
      "Epoch 73/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6775 - acc: 0.5822 - val_loss: 0.5419 - val_acc: 0.9919\n",
      "Epoch 74/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6775 - acc: 0.5821 - val_loss: 0.5449 - val_acc: 0.9982\n",
      "Epoch 75/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6772 - acc: 0.5825 - val_loss: 0.5809 - val_acc: 0.9781\n",
      "Epoch 76/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6772 - acc: 0.5824 - val_loss: 0.5620 - val_acc: 0.9897\n",
      "Epoch 77/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6775 - acc: 0.5824 - val_loss: 0.5579 - val_acc: 0.9947\n",
      "Epoch 78/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6774 - acc: 0.5823 - val_loss: 0.5540 - val_acc: 0.9899\n",
      "Epoch 79/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6773 - acc: 0.5824 - val_loss: 0.5560 - val_acc: 0.9960\n",
      "Epoch 80/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6771 - acc: 0.5824 - val_loss: 0.5738 - val_acc: 0.9723\n",
      "Epoch 81/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6770 - acc: 0.5825 - val_loss: 0.5624 - val_acc: 0.9976\n",
      "Epoch 82/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6770 - acc: 0.5825 - val_loss: 0.5772 - val_acc: 0.9837\n",
      "Epoch 83/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6774 - acc: 0.5823 - val_loss: 0.5650 - val_acc: 0.9825\n",
      "Epoch 84/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6772 - acc: 0.5825 - val_loss: 0.5431 - val_acc: 0.9849\n",
      "Epoch 85/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6774 - acc: 0.5823 - val_loss: 0.5594 - val_acc: 0.9896\n",
      "Epoch 86/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6775 - acc: 0.5824 - val_loss: 0.5533 - val_acc: 0.9905\n",
      "Epoch 87/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6774 - acc: 0.5823 - val_loss: 0.5386 - val_acc: 0.9959\n",
      "Epoch 88/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6765 - acc: 0.5831 - val_loss: 0.5607 - val_acc: 0.9769\n",
      "Epoch 110/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6762 - acc: 0.5835 - val_loss: 0.5593 - val_acc: 0.9821\n",
      "Epoch 111/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6762 - acc: 0.5834 - val_loss: 0.5587 - val_acc: 0.9846\n",
      "Epoch 112/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6760 - acc: 0.5834 - val_loss: 0.5577 - val_acc: 0.9845\n",
      "Epoch 113/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6765 - acc: 0.5832 - val_loss: 0.5502 - val_acc: 0.9916\n",
      "Epoch 114/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6763 - acc: 0.5831 - val_loss: 0.5627 - val_acc: 0.9772\n",
      "Epoch 115/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6756 - acc: 0.5836 - val_loss: 0.5491 - val_acc: 0.9917\n",
      "Epoch 116/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6763 - acc: 0.5833 - val_loss: 0.5407 - val_acc: 0.9916\n",
      "Epoch 117/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6757 - acc: 0.5840 - val_loss: 0.5547 - val_acc: 0.9745\n",
      "Epoch 118/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6761 - acc: 0.5835 - val_loss: 0.5627 - val_acc: 0.9849\n",
      "Epoch 119/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6756 - acc: 0.5840 - val_loss: 0.5589 - val_acc: 0.9859\n",
      "Epoch 120/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6757 - acc: 0.5838 - val_loss: 0.5553 - val_acc: 0.9929\n",
      "Epoch 121/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6761 - acc: 0.5834 - val_loss: 0.5566 - val_acc: 0.9859\n",
      "Epoch 122/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6763 - acc: 0.5831 - val_loss: 0.5407 - val_acc: 0.9952\n",
      "Epoch 123/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6757 - acc: 0.5839 - val_loss: 0.5534 - val_acc: 0.9883\n",
      "Epoch 124/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6758 - acc: 0.5838 - val_loss: 0.5462 - val_acc: 0.9893\n",
      "Epoch 125/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6757 - acc: 0.5836 - val_loss: 0.5505 - val_acc: 0.9881\n",
      "Epoch 126/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6762 - acc: 0.5835 - val_loss: 0.5606 - val_acc: 0.9755\n",
      "Epoch 127/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6755 - acc: 0.5840 - val_loss: 0.5484 - val_acc: 0.9856\n",
      "Epoch 128/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6759 - acc: 0.5838 - val_loss: 0.5589 - val_acc: 0.9840\n",
      "Epoch 129/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6754 - acc: 0.5839 - val_loss: 0.5514 - val_acc: 0.9849\n",
      "Epoch 130/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6751 - acc: 0.5842 - val_loss: 0.5493 - val_acc: 0.9871\n",
      "Epoch 131/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6752 - acc: 0.5841 - val_loss: 0.5617 - val_acc: 0.9804\n",
      "Epoch 132/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6757 - acc: 0.5839 - val_loss: 0.5460 - val_acc: 0.9894\n",
      "Epoch 133/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6751 - acc: 0.5843 - val_loss: 0.5468 - val_acc: 0.9794\n",
      "Epoch 134/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6758 - acc: 0.5839 - val_loss: 0.5395 - val_acc: 0.9871\n",
      "Epoch 135/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6748 - acc: 0.5846 - val_loss: 0.5463 - val_acc: 0.9822\n",
      "Epoch 136/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6752 - acc: 0.5843 - val_loss: 0.5498 - val_acc: 0.9880\n",
      "Epoch 137/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6748 - acc: 0.5844 - val_loss: 0.5475 - val_acc: 0.9856\n",
      "Epoch 138/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6750 - acc: 0.5843 - val_loss: 0.5412 - val_acc: 0.9814\n",
      "Epoch 139/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6747 - acc: 0.5845 - val_loss: 0.5493 - val_acc: 0.9830\n",
      "Epoch 140/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6752 - acc: 0.5841 - val_loss: 0.5539 - val_acc: 0.9816\n",
      "Epoch 141/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6755 - acc: 0.5841 - val_loss: 0.5526 - val_acc: 0.9859\n",
      "Epoch 142/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6758 - acc: 0.5837 - val_loss: 0.5504 - val_acc: 0.9843\n",
      "Epoch 143/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6751 - acc: 0.5842 - val_loss: 0.5578 - val_acc: 0.9837\n",
      "Epoch 144/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6751 - acc: 0.5840 - val_loss: 0.5599 - val_acc: 0.9835\n",
      "Epoch 145/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6755 - acc: 0.5840 - val_loss: 0.5688 - val_acc: 0.9806\n",
      "Epoch 146/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6751 - acc: 0.5844 - val_loss: 0.5583 - val_acc: 0.9816\n",
      "Epoch 147/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6753 - acc: 0.5841 - val_loss: 0.5578 - val_acc: 0.9755\n",
      "Epoch 148/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6753 - acc: 0.5840 - val_loss: 0.5449 - val_acc: 0.9907\n",
      "Epoch 149/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6748 - acc: 0.5846 - val_loss: 0.5573 - val_acc: 0.9833\n",
      "Epoch 150/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6748 - acc: 0.5848 - val_loss: 0.5589 - val_acc: 0.9705\n",
      "Epoch 151/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6742 - acc: 0.5849 - val_loss: 0.5444 - val_acc: 0.9796\n",
      "Epoch 153/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6748 - acc: 0.5847 - val_loss: 0.5483 - val_acc: 0.9866\n",
      "Epoch 154/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6750 - acc: 0.5845 - val_loss: 0.5496 - val_acc: 0.9830\n",
      "Epoch 155/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6743 - acc: 0.5849 - val_loss: 0.5638 - val_acc: 0.9690\n",
      "Epoch 156/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6744 - acc: 0.5848 - val_loss: 0.5591 - val_acc: 0.9785\n",
      "Epoch 157/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6749 - acc: 0.5845 - val_loss: 0.5566 - val_acc: 0.9804\n",
      "Epoch 158/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6743 - acc: 0.5848 - val_loss: 0.5582 - val_acc: 0.9843\n",
      "Epoch 159/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6743 - acc: 0.5848 - val_loss: 0.5487 - val_acc: 0.9843\n",
      "Epoch 160/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6742 - acc: 0.5849 - val_loss: 0.5648 - val_acc: 0.9666\n",
      "Epoch 161/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6744 - acc: 0.5849 - val_loss: 0.5564 - val_acc: 0.9845\n",
      "Epoch 162/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6743 - acc: 0.5849 - val_loss: 0.5513 - val_acc: 0.9808\n",
      "Epoch 163/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6745 - acc: 0.5849 - val_loss: 0.5660 - val_acc: 0.9816\n",
      "Epoch 164/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6741 - acc: 0.5851 - val_loss: 0.5605 - val_acc: 0.9850\n",
      "Epoch 165/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6739 - acc: 0.5852 - val_loss: 0.5648 - val_acc: 0.9725\n",
      "Epoch 166/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6739 - acc: 0.5852 - val_loss: 0.5600 - val_acc: 0.9796\n",
      "Epoch 167/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6742 - acc: 0.5850 - val_loss: 0.5618 - val_acc: 0.9857\n",
      "Epoch 168/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6739 - acc: 0.5854 - val_loss: 0.5443 - val_acc: 0.9868\n",
      "Epoch 169/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6748 - acc: 0.5845 - val_loss: 0.5588 - val_acc: 0.9861\n",
      "Epoch 170/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6734 - acc: 0.5856 - val_loss: 0.5584 - val_acc: 0.9840\n",
      "Epoch 171/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6738 - acc: 0.5855 - val_loss: 0.5574 - val_acc: 0.9765\n",
      "Epoch 172/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6743 - acc: 0.5851 - val_loss: 0.5485 - val_acc: 0.9835\n",
      "Epoch 173/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6736 - acc: 0.5855 - val_loss: 0.5676 - val_acc: 0.9659\n",
      "Epoch 174/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6742 - acc: 0.5850 - val_loss: 0.5533 - val_acc: 0.9821\n",
      "Epoch 175/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6740 - acc: 0.5852 - val_loss: 0.5556 - val_acc: 0.9839\n",
      "Epoch 176/200\n",
      "192/192 [==============================] - 64s 331ms/step - loss: 0.6738 - acc: 0.5853 - val_loss: 0.5579 - val_acc: 0.9768\n",
      "Epoch 177/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6740 - acc: 0.5854 - val_loss: 0.5617 - val_acc: 0.9843\n",
      "Epoch 178/200\n",
      "192/192 [==============================] - 63s 331ms/step - loss: 0.6740 - acc: 0.5853 - val_loss: 0.5506 - val_acc: 0.9786\n",
      "Epoch 179/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6734 - acc: 0.5855 - val_loss: 0.5522 - val_acc: 0.9786\n",
      "Epoch 180/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6734 - acc: 0.5857 - val_loss: 0.5610 - val_acc: 0.9742\n",
      "Epoch 181/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6739 - acc: 0.5852 - val_loss: 0.5473 - val_acc: 0.9819\n",
      "Epoch 182/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6737 - acc: 0.5854 - val_loss: 0.5554 - val_acc: 0.9804\n",
      "Epoch 183/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6735 - acc: 0.5857 - val_loss: 0.5554 - val_acc: 0.9722\n",
      "Epoch 184/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6736 - acc: 0.5854 - val_loss: 0.5564 - val_acc: 0.9783\n",
      "Epoch 185/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6736 - acc: 0.5856 - val_loss: 0.5596 - val_acc: 0.9809\n",
      "Epoch 186/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6749 - acc: 0.5843 - val_loss: 0.5585 - val_acc: 0.9880\n",
      "Epoch 187/200\n",
      "192/192 [==============================] - 63s 328ms/step - loss: 0.6741 - acc: 0.5850 - val_loss: 0.5644 - val_acc: 0.9768\n",
      "Epoch 188/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6736 - acc: 0.5856 - val_loss: 0.5614 - val_acc: 0.9821\n",
      "Epoch 189/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6735 - acc: 0.5857 - val_loss: 0.5686 - val_acc: 0.9613\n",
      "Epoch 190/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6734 - acc: 0.5858 - val_loss: 0.5606 - val_acc: 0.9765\n",
      "Epoch 191/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6736 - acc: 0.5855 - val_loss: 0.5446 - val_acc: 0.9752\n",
      "Epoch 192/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6734 - acc: 0.5858 - val_loss: 0.5408 - val_acc: 0.9817\n",
      "Epoch 193/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6742 - acc: 0.5850 - val_loss: 0.5561 - val_acc: 0.9843\n",
      "Epoch 194/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6734 - acc: 0.5856 - val_loss: 0.5615 - val_acc: 0.9774\n",
      "Epoch 195/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6733 - acc: 0.5858 - val_loss: 0.5593 - val_acc: 0.9830\n",
      "Epoch 196/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6737 - acc: 0.5856 - val_loss: 0.5531 - val_acc: 0.9744\n",
      "Epoch 197/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6745 - acc: 0.5849 - val_loss: 0.5594 - val_acc: 0.9734\n",
      "Epoch 198/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6737 - acc: 0.5853 - val_loss: 0.5562 - val_acc: 0.9762\n",
      "Epoch 199/200\n",
      "192/192 [==============================] - 63s 330ms/step - loss: 0.6736 - acc: 0.5856 - val_loss: 0.5557 - val_acc: 0.9764\n",
      "Epoch 200/200\n",
      "192/192 [==============================] - 63s 329ms/step - loss: 0.6737 - acc: 0.5855 - val_loss: 0.5659 - val_acc: 0.9770\n"
     ]
    }
   ],
   "source": [
    "# define the hyper param\n",
    "\n",
    "import tensorflow.keras as k\n",
    "steps_per_epoch = 3*(len(X_train))//batch_size  # depend on the training dataset\n",
    "\n",
    "callbacks = [\n",
    "    #k.callbacks.EarlyStopping(patience=5, monitor='val_loss'),\n",
    "    k.callbacks.TensorBoard(log_dir = 'logsAugH')\n",
    "]\n",
    "\n",
    "history = model.fit_generator(my_generator, validation_data=validation_datagen, \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    validation_steps=steps_per_epoch, epochs=200, callbacks=callbacks)  # use the model from blocks build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e60adc8cc390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accuracy'"
     ]
    }
   ],
   "source": [
    "# plot the accuracy and loss of the training\n",
    "\n",
    "# loss \n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IoU socre is:  0.0\n"
     ]
    }
   ],
   "source": [
    "# IOU \n",
    "y_pred=model.predict(X_test)\n",
    "y_pred_thresholded = y_pred > 0.5\n",
    "\n",
    "intersection = np.logical_and(Y_test, y_pred_thresholded)\n",
    "union = np.logical_or(Y_test, y_pred_thresholded)\n",
    "iou_score = np.sum(intersection) / np.sum(union)\n",
    "print(\"IoU socre is: \", iou_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 256, 256, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 256, 256, 1).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 249, in assert_input_compatibility\n        f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Exception encountered when calling layer \"U-Net\" (type Functional).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 256, 256, 1)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 256, 256, 1), dtype=float32)\n      • training=False\n      • mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7908/292713287.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_img_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtest_img_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_img_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_img_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"E:\\INSTALL\\anaconda\\envs\\seg\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 249, in assert_input_compatibility\n        f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Exception encountered when calling layer \"U-Net\" (type Functional).\n    \n    Input 0 of layer \"conv2d\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 256, 256, 1)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None, 256, 256, 1), dtype=float32)\n      • training=False\n      • mask=None\n"
     ]
    }
   ],
   "source": [
    "# on test dataset\n",
    "test_img_number = random.randint(0, len(X_test))\n",
    "test_img = X_test[test_img_number]\n",
    "ground_truth=Y_test[test_img_number]\n",
    "test_img_norm=test_img[:,:,0][:,:,None]\n",
    "test_img_input=np.expand_dims(test_img_norm, 0)\n",
    "prediction = (model.predict(test_img_input)[0,:,:,0] > 0.2).astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(231)\n",
    "plt.title('Testing Image')\n",
    "plt.imshow(test_img[:,:,0], cmap='gray')\n",
    "plt.subplot(232)\n",
    "plt.title('Testing Label')\n",
    "plt.imshow(ground_truth[:,:,0], cmap='gray')\n",
    "plt.subplot(233)\n",
    "plt.title('Prediction on test image')\n",
    "plt.imshow(prediction, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
